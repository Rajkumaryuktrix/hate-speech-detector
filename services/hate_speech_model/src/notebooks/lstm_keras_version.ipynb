{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, SpatialDropout1D, Dropout, add, concatenate\n",
    "from keras.layers import CuDNNLSTM, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing import text\n",
    "from keras.utils.data_utils import pad_sequences\n",
    "from keras.callbacks import LearningRateScheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download and unzip the common crawl, 300 dimensional word vectors from:\n",
    "\n",
    "https://fasttext.cc/docs/en/english-vectors.html\n",
    "\n",
    "https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "This notebook was adapted from:\n",
    "\n",
    "https://www.kaggle.com/thousandvoices/simple-lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 14\n",
    "DATA_PATH = '/'.join(os.getcwd().split(\"/\")[:-1]) + '/data/jigsaw_unintended_bias/'\n",
    "WORD_EMBEDDINGS = {\n",
    "    'fasttext': '../word_vectors/crawl-300d-2M.vec',\n",
    "    'glove': '../word_vectors/glove.840B.300d.txt'\n",
    "}\n",
    "TARGET_COLUMN = 'target'\n",
    "IDENTITY_COLUMNS = [\n",
    "    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n",
    "    'muslim', 'black', 'white', 'psychiatric_or_mental_illness'\n",
    "]\n",
    "NUM_MODELS = 1\n",
    "EMBED_DIM = 300\n",
    "LSTM_UNITS = 128\n",
    "DENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\n",
    "MAX_LEN = 220  # max word embeddings per document\n",
    "VOCAB_SIZE = 100000  # total distinct words or features - this limits the words to be embedded\n",
    "EPOCHS = 1\n",
    "BATCH_SIZE = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data: pd.Series):\n",
    "    \"\"\"\n",
    "    Cleans the text by removing special characters and returning a pd.Series of string type.\n",
    "    Credit goes to https://www.kaggle.com/gpreda/jigsaw-fast-compact-solution\n",
    "    \"\"\"\n",
    "    punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~`\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'\n",
    "\n",
    "    def clean_special_chars(text: str, punct: str):\n",
    "        \"\"\"Replaces the given characters, punct, in the string, text.\"\"\"\n",
    "        for p in punct:\n",
    "            text = text.replace(p, ' ')\n",
    "        return text\n",
    "\n",
    "    data = data.astype(str).apply(lambda x: clean_special_chars(x, punct))\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_coefs(word, *arr):\n",
    "    \"\"\"\n",
    "    Converts a line from the embedding file to a tuple of (word, 32-bit numpy array)\n",
    "\n",
    "    :param word: the first element in each line is the word\n",
    "    :param arr: elements 2-n are the embedding dimensions\n",
    "    \"\"\"\n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "\n",
    "def load_embeddings(path: str):\n",
    "    \"\"\"\n",
    "    Utility function to load word embeddings.  Each word embedding looks like:\n",
    "    word 0.3 0.4 0.5 0.6 ...\n",
    "    This function converts the embeddings to a dictionary of {word: numpy array}\n",
    "    \"\"\"\n",
    "    with open(path, 'r', encoding='UTF-8') as f:\n",
    "        return dict(get_coefs(*line.strip().split(' ')) for line in tqdm(f))\n",
    "\n",
    "\n",
    "def get_word_embeddings(word_index: dict, path: str):\n",
    "    \"\"\"\n",
    "    Maps words fround in the text (word_index) to their corresponding word embeddings from the \n",
    "    pre-trained model loaded from (path).  If any words cannot be found in the pre-trained model, \n",
    "    they are tracked in unknown_words.\n",
    "    \"\"\"\n",
    "    embedding_index = load_embeddings(path)\n",
    "    # create an empty matrix of shape (nbr_words, embed_dim)\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, EMBED_DIM))\n",
    "    unknown_words = []\n",
    "    \n",
    "    # map all words from the text to their embeddings, if they exist in the embedding index\n",
    "    for word, i in word_index.items():\n",
    "        try:\n",
    "            embedding_matrix[i] = embedding_index[word]\n",
    "        except KeyError:\n",
    "            unknown_words.append(word)\n",
    "    return embedding_matrix, unknown_words\n",
    "\n",
    "\n",
    "def build_model(embedding_matrix, num_aux_targets):\n",
    "    \"\"\"\n",
    "    Builds and compiles a model.\n",
    "    \"\"\"\n",
    "    word_vectors = Input(shape=(MAX_LEN,))\n",
    "\n",
    "    # create an embedding layer for the words of EMBED_DIM * NUM_MODELS dimensions\n",
    "    # this layer uses the pre-trained word embeddings\n",
    "    # instead of passing input_dim, output_dim explicitly, could also just pass *embedding_matrix.shape\n",
    "    x = Embedding(\n",
    "        input_dim = embedding_matrix.shape[0], \n",
    "        output_dim = embedding_matrix.shape[1], \n",
    "        weights=[embedding_matrix], \n",
    "        trainable=False\n",
    "    )(word_vectors)\n",
    "\n",
    "    # randomly drop features, i.e. [[1, 1, 1], [2, 1, 2]] -> [[1, 0, 1], [2, 0, 2]]\n",
    "    x = SpatialDropout1D(0.25)(x)\n",
    "\n",
    "    # each bidirectional layer outputs 2 sequences: 1 forward, 1 backward, and concatenates them\n",
    "    # so stacking 2 enriches the sequence features\n",
    "    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n",
    "    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n",
    "\n",
    "    # pool and reshape x from (batch_size, MAX_LEN, 2 * LSTM_UNITS) to hidden (BATCH_SIZE, 4 * LSTM_UNITS)\n",
    "    hidden = concatenate([\n",
    "        GlobalMaxPooling1D()(x),\n",
    "        GlobalAveragePooling1D()(x),\n",
    "    ])\n",
    "\n",
    "    # skip connections...\n",
    "    # add a product of a dense layer with the hidden layer to the output of the the hidden layer\n",
    "    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='tanh')(hidden)])\n",
    "    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n",
    "    result = Dense(1, activation='sigmoid', name='main_output')(hidden)\n",
    "\n",
    "    # auxiliary outputs to be predicted as an alternative to the main output\n",
    "    aux_result = Dense(num_aux_targets, activation='sigmoid', name='aux_ouput')(hidden)\n",
    "\n",
    "    model = Model(inputs=word_vectors, outputs=[result, aux_result])\n",
    "    print(model.summary())\n",
    "\n",
    "    model.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer=Adam(clipnorm=0.1),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expect ~2 Gb RAM for the data\n",
    "train_df = pd.read_csv(DATA_PATH + 'train.csv')\n",
    "test_df = pd.read_csv(DATA_PATH + 'test.csv')\n",
    "\n",
    "x_train = preprocess(train_df['comment_text'])\n",
    "y_train = train_df[TARGET_COLUMN]\n",
    "y_aux_train = train_df[['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']]\n",
    "x_test = preprocess(test_df['comment_text'])\n",
    "\n",
    "# binarize these variables as boolean values for sample weighting later\n",
    "# this conversion must follow the code above or it will give TF trouble converting the boolean array to a tensor\n",
    "for col in [TARGET_COLUMN] + IDENTITY_COLUMNS:\n",
    "    train_df[col] = np.where(train_df[col] >= 0.5, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Features (Vocab Size) 100000\n"
     ]
    }
   ],
   "source": [
    "tokenizer = text.Tokenizer(num_words=VOCAB_SIZE)\n",
    "tokenizer.fit_on_texts(list(x_train) + list(x_test))\n",
    "\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "x_test = tokenizer.texts_to_sequences(x_test)\n",
    "x_train = pad_sequences(x_train, maxlen=MAX_LEN)\n",
    "x_test = pad_sequences(x_test, maxlen=MAX_LEN)\n",
    "\n",
    "max_features = min(VOCAB_SIZE, len(tokenizer.word_index) + 1)\n",
    "print(f\"Max Features (Vocab Size) {max_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average of target variable: 0.0799690172277954\n",
      "Average of identity columns:\n",
      " male                             0.024647\n",
      "female                           0.029603\n",
      "homosexual_gay_or_lesbian        0.006093\n",
      "christian                        0.022397\n",
      "jewish                           0.004239\n",
      "muslim                           0.011638\n",
      "black                            0.008256\n",
      "white                            0.013897\n",
      "psychiatric_or_mental_illness    0.002709\n",
      "dtype: float64 \n",
      "\n",
      "222862\n",
      "1259449\n",
      "916525\n",
      "\n",
      " (1804874,) 0.42935264 21.03828\n"
     ]
    }
   ],
   "source": [
    "print(\"Average of target variable:\", train_df[TARGET_COLUMN].mean())\n",
    "print(\"Average of identity columns:\\n\", train_df[IDENTITY_COLUMNS].mean(), \"\\n\")\n",
    "\n",
    "# start off with uniform weights of 1\n",
    "sample_weights = np.ones(len(x_train), dtype=np.float32)\n",
    "\n",
    "# sum the binary identity columns to give rows with identity values more weight (they contain more information) \n",
    "sample_weights += train_df[IDENTITY_COLUMNS].sum(axis=1)\n",
    "change = train_df[IDENTITY_COLUMNS].sum(axis=1)\n",
    "print(change.sum())\n",
    "\n",
    "# when the target is 1, increase the weights further by counting the identity columns with 0 values\n",
    "# this increases the weights for the positive class (toxic comment)\n",
    "sample_weights += train_df[TARGET_COLUMN] * (~train_df[IDENTITY_COLUMNS]).sum(axis=1)\n",
    "change = train_df[TARGET_COLUMN] * (~train_df[IDENTITY_COLUMNS]).sum(axis=1)\n",
    "print(change.sum())\n",
    "\n",
    "# when the target is 0, increase the weights by counting the identity columns with value of 1\n",
    "# multiply by some constant (5) to give this weighting more impact\n",
    "# this increases the weights for the negative class (not a toxic comment)\n",
    "sample_weights += (~train_df[TARGET_COLUMN]) * train_df[IDENTITY_COLUMNS].sum(axis=1) * 5\n",
    "change = (~train_df[TARGET_COLUMN]) * train_df[IDENTITY_COLUMNS].sum(axis=1) * 5\n",
    "print(change.sum())\n",
    "\n",
    "# normalize the weights\n",
    "sample_weights /= sample_weights.mean()\n",
    "\n",
    "print(\"\\n\", sample_weights.shape, sample_weights.min(), sample_weights.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target is imbalanced.  The identity columns are sparse.  This weighting scheme makes 3 things happen:\n",
    "1. Observations with identity inforation are given a little more weight.\n",
    "\n",
    "2. Toxic comments are given a lot more weight (the amount of extra weight depends on missing identity information, which is a large number of rows - this is just a convient and arbitrary way of weighting toxic comments more heavily since most rows are missing identity information)\n",
    "\n",
    "3. Non-toxic comments are weighted even more when they do contain identity information.  This increase in weighting is scaled by 5, which is another arbitrary value.\n",
    "\n",
    "Weighting observations like this eliminates the need to re-sample the training data to balance the target and makes certain observations impact the model's error more than others.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1999996it [01:00, 33327.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown words (fast text):  173678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2196017it [01:06, 32896.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown words (glove):  170383\n"
     ]
    }
   ],
   "source": [
    "# create word embeddings\n",
    "\n",
    "fasttext_embeddings, fasttext_unknown_words = get_word_embeddings(tokenizer.word_index, WORD_EMBEDDINGS['fasttext'])\n",
    "print('Unknown words (fast text): ', len(fasttext_unknown_words))\n",
    "\n",
    "glove_embeddings, glove_unknown_words = get_word_embeddings(tokenizer.word_index, WORD_EMBEDDINGS['glove'])\n",
    "print('Unknown words (glove): ', len(glove_unknown_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix shape:  (327009, 600)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix = np.concatenate([fasttext_embeddings, glove_embeddings], axis=-1)\n",
    "print(\"Embedding matrix shape: \", embedding_matrix.shape)\n",
    "\n",
    "del fasttext_embeddings\n",
    "del glove_embeddings\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 220)]        0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 220, 600)     196205400   ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " spatial_dropout1d (SpatialDrop  (None, 220, 600)    0           ['embedding[0][0]']              \n",
      " out1D)                                                                                           \n",
      "                                                                                                  \n",
      " bidirectional (Bidirectional)  (None, 220, 256)     747520      ['spatial_dropout1d[0][0]']      \n",
      "                                                                                                  \n",
      " bidirectional_1 (Bidirectional  (None, 220, 256)    395264      ['bidirectional[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " global_max_pooling1d (GlobalMa  (None, 256)         0           ['bidirectional_1[0][0]']        \n",
      " xPooling1D)                                                                                      \n",
      "                                                                                                  \n",
      " global_average_pooling1d (Glob  (None, 256)         0           ['bidirectional_1[0][0]']        \n",
      " alAveragePooling1D)                                                                              \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 512)          0           ['global_max_pooling1d[0][0]',   \n",
      "                                                                  'global_average_pooling1d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 512)          262656      ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 512)          0           ['concatenate[0][0]',            \n",
      "                                                                  'dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 512)          262656      ['add[0][0]']                    \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 512)          0           ['add[0][0]',                    \n",
      "                                                                  'dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " main_output (Dense)            (None, 1)            513         ['add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " aux_ouput (Dense)              (None, 6)            3078        ['add_1[0][0]']                  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 197,877,087\n",
      "Trainable params: 1,671,687\n",
      "Non-trainable params: 196,205,400\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.001.\n",
      "3526/3526 [==============================] - 730s 205ms/step - loss: 0.5268 - main_output_loss: 0.4203 - aux_ouput_loss: 0.1065 - main_output_accuracy: 0.6935 - aux_ouput_accuracy: 0.9225 - lr: 0.0010\n",
      "191/191 [==============================] - 15s 77ms/step\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-5bfaff711493>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m submission = pd.DataFrame.from_dict({\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0;34m'id'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0;34m'prediction'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m })\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test' is not defined"
     ]
    }
   ],
   "source": [
    "def train_model(model, preds, weights):\n",
    "    \"\"\"\n",
    "    Train a model EPOCHS times.  After each epoch, reset the learning rate of the optimizer, using \n",
    "    a learning rate scheduler.\n",
    "    \"\"\"\n",
    "    for global_epoch in range(EPOCHS):\n",
    "        model.fit(\n",
    "            x_train,\n",
    "            [y_train, y_aux_train],\n",
    "            batch_size=BATCH_SIZE,\n",
    "            epochs=1,\n",
    "            verbose=1,\n",
    "            sample_weight=[sample_weights.values, np.ones_like(sample_weights)],\n",
    "            callbacks=[\n",
    "                LearningRateScheduler(lambda epoch: 1e-3 * (0.6 ** global_epoch), verbose=1)\n",
    "            ]\n",
    "        )\n",
    "        # although model has main_output and aux_output, only keep main_output (index 0)\n",
    "        checkpoint_predictions.append(\n",
    "            model.predict(x_test, batch_size=BATCH_SIZE)[0].flatten()\n",
    "        )\n",
    "        weights.append(2 ** global_epoch)\n",
    "\n",
    "\n",
    "checkpoint_predictions = []\n",
    "weights = []\n",
    "\n",
    "for model_idx in range(NUM_MODELS):\n",
    "    train_model(\n",
    "        model=build_model(embedding_matrix, y_aux_train.shape[-1]),\n",
    "        preds=checkpoint_predictions,\n",
    "        weights=weights\n",
    "    )\n",
    "\n",
    "# average the output of the NUM_MODELS models as the final predictions\n",
    "predictions = np.average(checkpoint_predictions, weights=weights, axis=0)\n",
    "\n",
    "submission = pd.DataFrame.from_dict({\n",
    "    'id': test_df['id'],\n",
    "    'prediction': predictions\n",
    "})\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7097326</td>\n",
       "      <td>Our oils read;  President IS taking different ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>7097339</td>\n",
       "      <td>Well here we go again.  Let's continue to subs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>7097346</td>\n",
       "      <td>Ignorance is bliss, ain't it?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>7097353</td>\n",
       "      <td>this is *&amp;^%ing outrageous. The prosecutor sho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>7097355</td>\n",
       "      <td>The profoundly stupid have spoken.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>7097356</td>\n",
       "      <td>The ignorance and bigotry comes from your post!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>7097369</td>\n",
       "      <td>An \"abject lesson\" is a lesson that is painful...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>7097371</td>\n",
       "      <td>Right on the money Gary Crum. And if they hide...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>7097402</td>\n",
       "      <td>Hey Dallas, Don't let the Iditarod get the bes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>7097403</td>\n",
       "      <td>I can't believe this country was so stupid.. -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>7097404</td>\n",
       "      <td>Somebody needs to dig up the Peterson's back y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>7097406</td>\n",
       "      <td>Only in America can you get 25 likes for admit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>7097432</td>\n",
       "      <td>Why? And facts, actual policies and action rat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>7097455</td>\n",
       "      <td>\"Just saw headlines on a couple of sites where...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>7097456</td>\n",
       "      <td>Who cares?  In all seriousness she, as with al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>7097459</td>\n",
       "      <td>that is our bully government for you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>7097474</td>\n",
       "      <td>If a News person dies for ratings are we suppo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>7097486</td>\n",
       "      <td>Sometimes it's the way you say it. Just as Oba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>7097520</td>\n",
       "      <td>Trump doesn't need no damn intelligence...he's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>7097544</td>\n",
       "      <td>Sounds more like the Antifascists are the real...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>7097578</td>\n",
       "      <td>Lying slut slut slut, wants money now</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>7097584</td>\n",
       "      <td>You hit the nail square on the head and pounde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>7097614</td>\n",
       "      <td>Dictionary definition:  Militia 1.(a) org., an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>7097617</td>\n",
       "      <td>\"hadn't been out hunting it...\" he said!\\nbut ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>7097621</td>\n",
       "      <td>Well its official  51 vs 49.  Yea!  Victory Vi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                       comment_text\n",
       "6    7097326  Our oils read;  President IS taking different ...\n",
       "19   7097339  Well here we go again.  Let's continue to subs...\n",
       "26   7097346                      Ignorance is bliss, ain't it?\n",
       "33   7097353  this is *&^%ing outrageous. The prosecutor sho...\n",
       "35   7097355                 The profoundly stupid have spoken.\n",
       "36   7097356    The ignorance and bigotry comes from your post!\n",
       "49   7097369  An \"abject lesson\" is a lesson that is painful...\n",
       "51   7097371  Right on the money Gary Crum. And if they hide...\n",
       "82   7097402  Hey Dallas, Don't let the Iditarod get the bes...\n",
       "83   7097403  I can't believe this country was so stupid.. -...\n",
       "84   7097404  Somebody needs to dig up the Peterson's back y...\n",
       "86   7097406  Only in America can you get 25 likes for admit...\n",
       "112  7097432  Why? And facts, actual policies and action rat...\n",
       "135  7097455  \"Just saw headlines on a couple of sites where...\n",
       "136  7097456  Who cares?  In all seriousness she, as with al...\n",
       "139  7097459               that is our bully government for you\n",
       "154  7097474  If a News person dies for ratings are we suppo...\n",
       "166  7097486  Sometimes it's the way you say it. Just as Oba...\n",
       "200  7097520  Trump doesn't need no damn intelligence...he's...\n",
       "224  7097544  Sounds more like the Antifascists are the real...\n",
       "258  7097578              Lying slut slut slut, wants money now\n",
       "264  7097584  You hit the nail square on the head and pounde...\n",
       "294  7097614  Dictionary definition:  Militia 1.(a) org., an...\n",
       "297  7097617  \"hadn't been out hunting it...\" he said!\\nbut ...\n",
       "301  7097621  Well its official  51 vs 49.  Yea!  Victory Vi..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.loc[np.where(predictions>0.5)].head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
