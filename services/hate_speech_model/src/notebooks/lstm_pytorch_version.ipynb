{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import gc\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils import data\n",
    "from torch.nn import functional as F\n",
    "from tqdm.notebook import tqdm_notebook as tqdm\n",
    "from keras.preprocessing import text\n",
    "from keras.utils.data_utils import pad_sequences\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download and unzip the common crawl, 300 dimensional word vectors from:\n",
    "\n",
    "https://fasttext.cc/docs/en/english-vectors.html\n",
    "\n",
    "https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "This notebook was adapted from:\n",
    "\n",
    "https://www.kaggle.com/code/bminixhofer/deterministic-neural-networks-using-pytorch/notebook\n",
    "\n",
    "https://www.kaggle.com/code/bminixhofer/simple-lstm-pytorch-version/notebook\n",
    "\n",
    "https://www.kaggle.com/code/shujian/single-rnn-with-4-folds-clr/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 14\n",
    "DATA_PATH = '/'.join(os.getcwd().split(\"/\")[:-1]) + '/data/jigsaw_unintended_bias/'\n",
    "WORD_EMBEDDINGS = {\n",
    "    'fasttext': '../word_vectors/crawl-300d-2M.vec',\n",
    "    'glove': '../word_vectors/glove.840B.300d.txt'\n",
    "}\n",
    "TARGET_COLUMN = 'target'\n",
    "IDENTITY_COLUMNS = [\n",
    "    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n",
    "    'muslim', 'black', 'white', 'psychiatric_or_mental_illness'\n",
    "]\n",
    "NUM_MODELS = 1\n",
    "EMBED_DIM = 300\n",
    "LSTM_UNITS = 128\n",
    "DENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\n",
    "MAX_LEN = 220  # max word embeddings per document\n",
    "VOCAB_SIZE = 100000  # total distinct words or features - this limits the words to be embedded\n",
    "EPOCHS = 1\n",
    "BATCH_SIZE = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int):\n",
    "    \"\"\"Ensures experiment will run deterministically for any given seed, even with CUDA\"\"\"\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "def preprocess(data: pd.Series):\n",
    "    \"\"\"\n",
    "    Cleans the text by removing special characters and returning a pd.Series of string type.\n",
    "    Credit goes to https://www.kaggle.com/gpreda/jigsaw-fast-compact-solution\n",
    "    \"\"\"\n",
    "    punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~`\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'\n",
    "\n",
    "    def clean_special_chars(text: str, punct: str):\n",
    "        \"\"\"Replaces the given characters, punct, in the string, text.\"\"\"\n",
    "        for p in punct:\n",
    "            text = text.replace(p, ' ')\n",
    "        return text\n",
    "\n",
    "    data = data.astype(str).apply(lambda x: clean_special_chars(x, punct))\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_coefs(word, *arr):\n",
    "    \"\"\"\n",
    "    Converts a line from the embedding file to a tuple of (word, 32-bit numpy array)\n",
    "\n",
    "    :param word: the first element in each line is the word\n",
    "    :param arr: elements 2-n are the embedding dimensions\n",
    "    \"\"\"\n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "\n",
    "def load_embeddings(path: str):\n",
    "    \"\"\"\n",
    "    Utility function to load word embeddings.  Each word embedding looks like:\n",
    "    word 0.3 0.4 0.5 0.6 ...\n",
    "    This function converts the embeddings to a dictionary of {word: numpy array}\n",
    "    \"\"\"\n",
    "    with open(path, 'r', encoding='UTF-8') as f:\n",
    "        return dict(get_coefs(*line.strip().split(' ')) for line in tqdm(f))\n",
    "\n",
    "\n",
    "def get_word_embeddings(word_index: dict, path: str):\n",
    "    \"\"\"\n",
    "    Maps words fround in the text (word_index) to their corresponding word embeddings from the \n",
    "    pre-trained model loaded from (path).  If any words cannot be found in the pre-trained model, \n",
    "    they are tracked in unknown_words.\n",
    "    \"\"\"\n",
    "    embedding_index = load_embeddings(path)\n",
    "    # create an empty matrix of shape (nbr_words, embed_dim)\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, EMBED_DIM))\n",
    "    unknown_words = []\n",
    "    \n",
    "    # map all words from the text to their embeddings, if they exist in the embedding index\n",
    "    for word, i in word_index.items():\n",
    "        try:\n",
    "            embedding_matrix[i] = embedding_index[word]\n",
    "        except KeyError:\n",
    "            unknown_words.append(word)\n",
    "    return embedding_matrix, unknown_words\n",
    "\n",
    "\n",
    "def sigmoid(x: np.ndarray):\n",
    "    \"\"\"Sigmoid activation function\"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model, \n",
    "    train, \n",
    "    test, \n",
    "    loss_fn, \n",
    "    output_dim: int, \n",
    "    lr: float = 0.001,\n",
    "    batch_size: int = 512, \n",
    "    n_epochs: int = 4, \n",
    "    enable_checkpoint_ensemble: bool = True\n",
    "):\n",
    "    \"\"\"\n",
    "    Trains a model on the training set.  \n",
    "    \"\"\"\n",
    "    param_lrs = [{'params': param, 'lr': lr} for param in model.parameters()]\n",
    "    optimizer = torch.optim.Adam(param_lrs, lr=lr)\n",
    "\n",
    "    # decay the learning rate using a schedule of 0.6^epoch\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda epoch: 0.6 ** epoch)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)\n",
    "    all_test_preds = []\n",
    "    checkpoint_weights = [2 ** epoch for epoch in range(n_epochs)]\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # increment learning rate schedule\n",
    "        scheduler.step()\n",
    "        \n",
    "        model.train()\n",
    "        avg_loss = 0.\n",
    "        \n",
    "        for data in tqdm(train_loader, disable=False):\n",
    "            # the target is the final column in data\n",
    "            x_batch = data[:-1]\n",
    "            y_batch = data[-1]\n",
    "\n",
    "            # forward pass and calculate loss\n",
    "            y_pred = model(*x_batch)            \n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "\n",
    "            # zero out the gradients for the optimizer, now that the loss has been calculated\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # backpropagate the loss\n",
    "            loss.backward()\n",
    "\n",
    "            # update the weights using the optimizer\n",
    "            optimizer.step()\n",
    "            \n",
    "            # track the mean loss over all batches in this epoch\n",
    "            avg_loss += loss.item() / len(train_loader)\n",
    "            \n",
    "        model.eval()\n",
    "        test_preds = np.zeros((len(test), output_dim))\n",
    "\n",
    "        # run each batch of the test data through the model\n",
    "        for i, x_batch in enumerate(test_loader):\n",
    "            y_pred = sigmoid(model(*x_batch).detach().cpu().numpy())\n",
    "            test_preds[i * batch_size:(i+1) * batch_size, :] = y_pred\n",
    "\n",
    "        # append the batch test_preds to the epoch's all_test_preds\n",
    "        all_test_preds.append(test_preds)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f'Epoch {epoch + 1}/{n_epochs} \\t loss={avg_loss:.4f} \\t time={elapsed_time:.2f}s')\n",
    "\n",
    "    if enable_checkpoint_ensemble:\n",
    "        test_preds = np.average(all_test_preds, weights=checkpoint_weights, axis=0)    \n",
    "    else:\n",
    "        test_preds = all_test_preds[-1]\n",
    "        \n",
    "    return test_preds\n",
    "\n",
    "\n",
    "def threshold_search(y_true, y_proba):\n",
    "    \"\"\"Finds the best probability threshold to maximize F1 score\"\"\"\n",
    "    best_threshold = 0\n",
    "    best_score = 0\n",
    "    for threshold in tqdm([i * 0.01 for i in range(100)]):\n",
    "        score = f1_score(y_true=y_true, y_pred=y_proba > threshold)\n",
    "        if score > best_score:\n",
    "            best_threshold = threshold\n",
    "            best_score = score\n",
    "    search_result = {'threshold': best_threshold, 'f1': best_score}\n",
    "    return search_result\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements an attention module.\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_dim: int, step_dim: int, bias: bool = True, **kwargs):\n",
    "        \"\"\"\n",
    "        Builds the decoder piece of attention module.  The encoder piece is assumed to have \n",
    "        been built with either a bi-directional RNN or self attention. \n",
    "        \n",
    "        :param feature_dim: the number of features, or input layer size\n",
    "        :param step_dim: the max sequence length\n",
    "        \"\"\"\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "        \n",
    "        self.supports_masking = True\n",
    "        self.bias = bias\n",
    "        self.feature_dim = feature_dim\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        \n",
    "        # initialize weights\n",
    "        weight = torch.zeros(feature_dim, 1)\n",
    "        nn.init.xavier_uniform_(weight)\n",
    "        self.weight = nn.Parameter(weight)\n",
    "        \n",
    "        if bias:\n",
    "            self.b = nn.Parameter(torch.zeros(step_dim))\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Implements forward pass through attention module\n",
    "        \n",
    "        :param x: the encoded input vector from a bi-directional RNN, or in other words \n",
    "            the concatenated foward and backward hidden states, h_j, from the equations \n",
    "            for attention.  For help understanding this, \n",
    "            see: https://bgg.medium.com/seq2seq-pay-attention-to-self-attention-part-1-d332e85e9aad\n",
    "        \"\"\"\n",
    "        feature_dim = self.feature_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        # alignment vector that scores how well the inputs at position j match the output at position i\n",
    "        # this is e_ij = a(s_i-1, h_j), \n",
    "        #   where s_i-1 is the decoder hidden states (self.weight) and h_j is the jth input label (x)\n",
    "        eij = torch.mm(\n",
    "            x.contiguous().view(-1, feature_dim), \n",
    "            self.weight\n",
    "        ).view(-1, step_dim)\n",
    "        \n",
    "        if self.bias:\n",
    "            eij = eij + self.b\n",
    "            \n",
    "        eij = torch.tanh(eij)\n",
    "\n",
    "        # the attention score, a_ij, is just 'a' here, and the next line computs the numerator of a_ij\n",
    "        a = torch.exp(eij)\n",
    "        \n",
    "        # if masked, multiply the attention score by hidden states of the input sequence\n",
    "        if mask is not None:\n",
    "            a = a * mask\n",
    "\n",
    "        # finalize computation of a_ij\n",
    "        a = a / torch.sum(a, 1, keepdim=True) + 1e-10\n",
    "\n",
    "        # weight the input by multiplying it by the attention score, a_ij * h_j\n",
    "        weighted_input = x * torch.unsqueeze(a, -1)\n",
    "        \n",
    "        # sum the weighted input to return the context vector, ci\n",
    "        return torch.sum(weighted_input, 1)\n",
    "\n",
    "\n",
    "class SpatialDropout(nn.Dropout2d):\n",
    "    \"\"\"\n",
    "    Implements the functionality of Keras' SpatialDropout1D.\n",
    "    Randomly drop features, i.e. [[1, 1, 1], [2, 1, 2]] -> [[1, 0, 1], [2, 0, 2]]\n",
    "    Compare this with ordinary dropout that drops by sample, i.e. [[1, 1, 1], [2, 1, 2]] -> [[1, 0, 1], [0, 1, 2]]\n",
    "    \"\"\"\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(2)    # add a dimension of size 1 at position 2, producing (N, T, 1, K)\n",
    "        x = x.permute(0, 3, 2, 1)  # re-order dimensions to (N, K, 1, T)\n",
    "        x = super(SpatialDropout, self).forward(x)  # (N, K, 1, T), some features are masked\n",
    "        x = x.permute(0, 3, 2, 1)  # re-order dimensions to (N, T, 1, K)\n",
    "        x = x.squeeze(2)  # remove dimension of size 1 at position 2, producing (N, T, K)\n",
    "        return x\n",
    "\n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, embedding_matrix: np.ndarray, num_aux_targets: int):\n",
    "        \"\"\"Sets up neural network architecture\"\"\"\n",
    "        super(NeuralNet, self).__init__()\n",
    "        \n",
    "        # set up a non-trainable, pre-trained embedding layer from the provided embedding_matrix\n",
    "        self.embedding = nn.Embedding(VOCAB_SIZE, EMBED_DIM)\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        self.embedding_dropout = SpatialDropout(0.3)  # randomly drop this percent of features\n",
    "        \n",
    "        # each bidirectional layer outputs 2 sequences: 1 forward, 1 backward, and concatenates them\n",
    "        # so stacking 2 enriches the sequence features\n",
    "        self.lstm1 = nn.LSTM(\n",
    "            input_size=EMBED_DIM, \n",
    "            hidden_size=LSTM_UNITS, \n",
    "            bidirectional=True, \n",
    "            batch_first=True\n",
    "        )\n",
    "        self.lstm2 = nn.LSTM(\n",
    "            input_size=LSTM_UNITS * 2, \n",
    "            hidden_size=LSTM_UNITS, \n",
    "            bidirectional=True, \n",
    "            batch_first=True\n",
    "        )\n",
    "    \n",
    "        # skip connections...\n",
    "        # add a product of a dense layer with the hidden layer to the output of the the hidden layer\n",
    "        self.linear1 = nn.Linear(in_features=DENSE_HIDDEN_UNITS, out_features=DENSE_HIDDEN_UNITS, bias=True)\n",
    "        self.linear2 = nn.Linear(in_features=DENSE_HIDDEN_UNITS, out_features=DENSE_HIDDEN_UNITS, bias=True)\n",
    "        \n",
    "        self.linear_out = nn.Linear(DENSE_HIDDEN_UNITS, 1)\n",
    "        \n",
    "        # auxiliary outputs to be predicted as an alternative to the main output\n",
    "        self.linear_aux_out = nn.Linear(DENSE_HIDDEN_UNITS, num_aux_targets)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Implements forward pass\"\"\"\n",
    "        h_embedding = self.embedding(x)\n",
    "        h_embedding = self.embedding_dropout(h_embedding)\n",
    "        \n",
    "        h_lstm1, _ = self.lstm1(h_embedding)\n",
    "        h_lstm2, _ = self.lstm2(h_lstm1)\n",
    "        \n",
    "        # attenion module can be inserted here, as an attention module's encoder uses a bi-directional RNN, \n",
    "        #   which was just defined above\n",
    "        # atten_1 = Attention(LSTM_UNITS * 2, MAX_LEN)(h_lstm1)  # skip connection\n",
    "        # atten_2 = Attention(LSTM_UNITS * 2, MAX_LEN)(h_lstm2)\n",
    "\n",
    "        avg_pool = torch.mean(h_lstm2, 1)  # global mean pooling\n",
    "        max_pool, _ = torch.max(h_lstm2, 1)  # global max pooling\n",
    "\n",
    "        # concatenate to reshape from (batch_size, MAX_LEN, LSTM_UNITS * 2) to h_conc (BATCH_SIZE, LSTM_UNITS * 4)\n",
    "        # if using attention, un-comment the next line and comment out the line after\n",
    "        # h_conc = torch.cat((atten_1, atten_2, max_pool, avg_pool), 1)\n",
    "        h_conc = torch.cat((max_pool, avg_pool), 1)\n",
    "        h_conc_linear1  = F.relu(self.linear1(h_conc))\n",
    "        h_conc_linear2  = F.relu(self.linear2(h_conc))\n",
    "        \n",
    "        hidden = h_conc + h_conc_linear1 + h_conc_linear2\n",
    "        \n",
    "        result = self.linear_out(hidden)\n",
    "        aux_result = self.linear_aux_out(hidden)\n",
    "        out = torch.cat([result, aux_result], 1)\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expect ~2 Gb RAM for the data\n",
    "train_df = pd.read_csv(DATA_PATH + 'train.csv')\n",
    "test_df = pd.read_csv(DATA_PATH + 'test.csv')\n",
    "\n",
    "x_train = preprocess(train_df['comment_text'])\n",
    "y_train = np.where(train_df['target'] >= 0.5, 1, 0)  # binarize the target\n",
    "y_aux_train = train_df[['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']]\n",
    "x_test = preprocess(test_df['comment_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Features (Vocab Size) 100000\n"
     ]
    }
   ],
   "source": [
    "tokenizer = text.Tokenizer(num_words=VOCAB_SIZE)\n",
    "tokenizer.fit_on_texts(list(x_train) + list(x_test))\n",
    "\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "x_test = tokenizer.texts_to_sequences(x_test)\n",
    "x_train = pad_sequences(x_train, maxlen=MAX_LEN)\n",
    "x_test = pad_sequences(x_test, maxlen=MAX_LEN)\n",
    "\n",
    "max_features = min(VOCAB_SIZE, len(tokenizer.word_index) + 1)\n",
    "print(f\"Max Features (Vocab Size) {max_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60fc3f4538bc4ffcb53f7c6f99121c40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown words (fast text):  173678\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b45cae06be1d4ea8ba3af23b0839c215",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown words (glove):  170383\n"
     ]
    }
   ],
   "source": [
    "# create word embeddings\n",
    "\n",
    "fasttext_embeddings, fasttext_unknown_words = get_word_embeddings(tokenizer.word_index, WORD_EMBEDDINGS['fasttext'])\n",
    "print('Unknown words (fast text): ', len(fasttext_unknown_words))\n",
    "\n",
    "glove_embeddings, glove_unknown_words = get_word_embeddings(tokenizer.word_index, WORD_EMBEDDINGS['glove'])\n",
    "print('Unknown words (glove): ', len(glove_unknown_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix shape:  (327009, 600)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix = np.concatenate([fasttext_embeddings, glove_embeddings], axis=-1)\n",
    "print(\"Embedding matrix shape: \", embedding_matrix.shape)\n",
    "\n",
    "del fasttext_embeddings\n",
    "del glove_embeddings\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move data to CUDA\n",
    "x_train_torch = torch.tensor(x_train, dtype=torch.long)#.cuda()\n",
    "x_test_torch = torch.tensor(x_test, dtype=torch.long)#.cuda()\n",
    "y_train_torch = torch.tensor(np.hstack([y_train[:, np.newaxis], y_aux_train]), dtype=torch.float32)#.cuda()\n",
    "\n",
    "# convert to tensor datasets\n",
    "train_dataset = data.TensorDataset(x_train_torch, y_train_torch)\n",
    "test_dataset = data.TensorDataset(x_test_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model  0\n"
     ]
    }
   ],
   "source": [
    "all_test_preds = []\n",
    "\n",
    "# train NUM_MODELS models and average their output for the final predictions\n",
    "for model_idx in range(NUM_MODELS):\n",
    "    print('\\nModel ', model_idx)\n",
    "\n",
    "    # fit each model with a different seed, otherwise they will be identical\n",
    "    seed_everything(SEED + model_idx)\n",
    "    \n",
    "    model = NeuralNet(embedding_matrix, y_aux_train.shape[-1])\n",
    "    #model.cuda()\n",
    "    \n",
    "    test_preds = train_model(\n",
    "        model, \n",
    "        train_dataset, \n",
    "        test_dataset, \n",
    "        output_dim=y_train_torch.shape[-1],\n",
    "        loss_fn=nn.BCEWithLogitsLoss(reduction='mean')\n",
    "    )\n",
    "    all_test_preds.append(test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame.from_dict({\n",
    "    'id': test_df['id'],\n",
    "    'prediction': np.mean(all_test_preds, axis=0)[:, 0]\n",
    "})\n",
    "\n",
    "submission.to_csv('pt_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.loc[np.where(np.mean(all_test_preds, axis=0)[:, 0]>0.5)].head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample weighting was not used here.\n",
    "\n",
    "## Notes from the author\n",
    "\n",
    "Note that the solution is not validated in this kernel. So for tuning anything, you should build a validation framework using e. g. KFold CV. If you just check what works best by submitting, you are very likely to overfit to the public leader board.\n",
    "\n",
    "\n",
    "### Ways to improve this kernel\n",
    "\n",
    "This kernel is just a simple baseline kernel, so there are many ways to improve it. Some ideas to get you started:\n",
    "\n",
    "* Add a contraction mapping. E. g. mapping \"is'nt\" to \"is not\" can help the network because \"not\" is explicitly mentioned. They were very popular in the recent quora competition, see for example this kernel: https://www.kaggle.com/theoviel/improve-your-score-with-some-text-preprocessing.\n",
    "* Try to reduce the number of words that are not found in the embeddings. At the moment, around 170k words are not found. We can take some steps to decrease this amount, for example trying to find a vector for a processed (capitalized, stemmed, ...) version of the word when the vector for the regular word can not be found. See the 3rd place solution of the quora competition (https://www.kaggle.com/wowfattie/3rd-place) for an excellent implementation of this.\n",
    "* Try cyclic learning rate (CLR). I have found CLR to almost always improve my network recently compared to the default parameters for Adam. In this case, we are already using a learning rate scheduler, so this might not be the case. But it is still worth to try it out. See for example my my other PyTorch kernel (https://www.kaggle.com/bminixhofer/deterministic-neural-networks-using-pytorch) for an implementation of CLR in PyTorch.\n",
    "* Use sequence bucketing to train faster and fit more networks into the two hours. The winning team of the quora competition (https://www.kaggle.com/c/quora-insincere-questions-classification/discussion/80568#latest-487092) successfully used sequence bucketing to drastically reduce the time it took to train RNNs. An excerpt from their solution summary:\n",
    "\n",
    "\"\n",
    "We aimed at combining as many models as possible. To do this, we needed to improve runtime and the most important thing to achieve this was the following. We do not pad sequences to the same length based on the whole data, but just on a batch level. That means we conduct padding and truncation on the data generator level for each batch separately, so that length of the sentences in a batch can vary in size. Additionally, we further improved this by not truncating based on the length of the longest sequence in the batch, but based on the 95% percentile of lengths within the sequence. This improved runtime heavily and kept accuracy quite robust on single model level, and improved it by being able to average more models.\n",
    "\"\n",
    "\n",
    "* Try a (weighted) average of embeddings instead of concatenating them. A 600d vector for each word is a lot, it might work better to average them instead. See this paper for why this even works (https://www.aclweb.org/anthology/N18-2031).\n",
    "* Limit the maximum number of words used to train the NN. At the moment, there is no limit set to the maximum number of words in the tokenizer, so we use every word that occurs in the training data, even if it is only mentioned once. This could lead to overfitting so it might be better to limit the maximum number of words to e. g. 100k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
